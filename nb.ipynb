{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install git-lfs\n",
    "!apt-get install festival espeak-ng mbrola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y git-lfs festival espeak-ng mbrola\n",
    "%cd HierSpeech_TTS\n",
    "!pip install -r requirements.txt\n",
    "!pip install gradio \n",
    "!pip install utils\n",
    "!python app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "import torchaudio\n",
    "import utils\n",
    "from Mels_preprocess import MelSpectrogramFixed\n",
    "from hierspeechpp_speechsynthesizer import SynthesizerTrn\n",
    "from ttv_v1.text import text_to_sequence\n",
    "from ttv_v1.t2w2v_transformer import SynthesizerTrn as Text2W2V\n",
    "from speechsr24k.speechsr import SynthesizerTrn as AudioSR\n",
    "from speechsr48k.speechsr import SynthesizerTrn as AudioSR48\n",
    "from denoiser.generator import MPNet\n",
    "from denoiser.infer import denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the TTS function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts(text, a, hierspeech):\n",
    "    net_g, text2w2v, audiosr, denoiser, mel_fn = hierspeech\n",
    "    os.makedirs(a.output_dir, exist_ok=True)\n",
    "    text = text_to_sequence(str(text), [\"english_cleaners2\"])\n",
    "    token = add_blank_token(text).unsqueeze(0).cuda()\n",
    "    token_length = torch.LongTensor([token.size(-1)]).cuda()Â \n",
    "    # Prompt load\n",
    "    audio, sample_rate = torchaudio.load(a.input_prompt)\n",
    "    # support only single channel\n",
    "    audio = audio[:1,:] \n",
    "    # Resampling\n",
    "    if sample_rate != 16000:\n",
    "        audio = torchaudio.functional.resample(audio, sample_rate, 16000, resampling_method=\"kaiser_window\") \n",
    "    if a.scale_norm == 'prompt':\n",
    "        prompt_audio_max = torch.max(audio.abs())\n",
    "# We utilize a hop size of 320 but denoiser uses a hop size of 400 so we utilize a hop size of 1600\n",
    "    ori_prompt_len = audio.shape[-1]\n",
    "    p = (ori_prompt_len // 1600 + 1) * 1600 - ori_prompt_len\n",
    "    audio = torch.nn.functional.pad(audio, (0, p), mode='constant').data\n",
    "    file_name = os.path.splitext(os.path.basename(a.input_prompt))[0]\n",
    "    # If you have a memory issue during denosing the prompt, try to denoise the prompt with cpu before TTS \n",
    "    # We will have a plan to replace a memory-efficient denoiser \n",
    "    if a.denoise_ratio == 0:\n",
    "        audio = torch.cat([audio.cuda(), audio.cuda()], dim=0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            denoised_audio = denoise(audio.squeeze(0).cuda(), denoiser, hps_denoiser)\n",
    "        audio = torch.cat([audio.cuda(), denoised_audio[:,:audio.shape[-1]]], dim=0)\n",
    "        audio = audio[:,:ori_prompt_len]  # 20231108 We found that large size of padding decreases a performance so we remove the paddings after denosing.\n",
    "        src_mel = mel_fn(audio.cuda())\n",
    "        src_length = torch.LongTensor([src_mel.size(2)]).to(device)\n",
    "        src_length2 = torch.cat([src_length,src_length], dim=0)\n",
    "    ## TTV (Text --> W2V, F0)\n",
    "    with torch.no_grad():\n",
    "        w2v_x, pitch = text2w2v.infer_noise_control(token, token_length, src_mel, src_length2, noise_scale=a.noise_scale_ttv, denoise_ratio=a.denoise_ratio)\n",
    "        src_length = torch.LongTensor([w2v_x.size(2)]).cuda()  \n",
    "        \n",
    "        ## Pitch Clipping\n",
    "        pitch[pitch<torch.log(torch.tensor([55]).cuda())]  = 0\n",
    "\n",
    "        ## Hierarchical Speech Synthesizer (W2V, F0 --> 16k Audio)\n",
    "        converted_audio = \\\n",
    "            net_g.voice_conversion_noise_control(w2v_x, src_length, src_mel, src_length2, pitch, noise_scale=a.noise_scale_vc, denoise_ratio=a.denoise_ratio)\n",
    "                \n",
    "        ## SpeechSR (Optional) (16k Audio --> 24k or 48k Audio)\n",
    "        if a.output_sr == 48000 or 24000:\n",
    "            converted_audio = audiosr(converted_audio)\n",
    "       converted_audio = converted_audio.squeeze()\n",
    "    \n",
    "    if a.scale_norm == 'prompt':\n",
    "        converted_audio = converted_audio / (torch.abs(converted_audio).max()) * 32767.0 * prompt_audio_max\n",
    "    else:\n",
    "        converted_audio = converted_audio / (torch.abs(converted_audio).max()) * 32767.0 * 0.999 \n",
    "        converted_audio = converted_audio.cpu().numpy().astype('int16')\n",
    "    file_name2 = \"{}.wav\".format(file_name)\n",
    "    output_file = os.path.join(a.output_dir, file_name2)\n",
    "    if a.output_sr == 48000:\n",
    "        write(output_file, 48000, converted_audio)\n",
    "    elif a.output_sr == 24000:\n",
    "        write(output_file, 24000, converted_audio)\n",
    "    else:\n",
    "        write(output_file, 16000, converted_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd HierSpeech_TTS\n",
    "!python app.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
